# -*- coding: utf-8 -*-
"""Task_1_house_price_prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1B0A23aiZx5CtUWFXaFmviC6Gva8mf5Vb

###Problem Statement:
Machine Learning model to predict house price using linear regression

##Importing dependencies :

*   Numpy
*   Pandas
*   Matplotlib
*   Seaborn
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

"""#Importing dataset and train_test_split from sklearn"""

import sklearn.datasets
import sklearn.linear_model
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error,r2_score

"""#Importing dataset


"""

house_price_dataset=pd.read_csv(r"Housing.csv")
print(house_price_dataset)

"""#Data Inspection

"""

house_price_dataset.shape

house_price_dataset.info()

house_price_dataset.describe()

"""#Data Cleaning


"""

# Checking Null values
house_price_dataset.isnull().sum()*100/house_price_dataset.shape[0]
# There are no NULL values in the dataset, hence it is clean.

"""#Data Preparation"""

varlist =  ['mainroad', 'guestroom', 'basement', 'hotwaterheating', 'airconditioning', 'prefarea']

# Defining the map function
def binary_map(x):
    return x.map({'yes': 1, "no": 0})

# Applying the function to the housing list
house_price_dataset[varlist] = house_price_dataset[varlist].apply(binary_map)

# Check the housing dataframe now

house_price_dataset.head()

"""#Dummy Variables

The variable furnishingstatus has three levels. We need to convert these levels into integer as well.

For this, we will use something called dummy variables.
"""

# Get the dummy variables for the feature 'furnishingstatus' and store it in a new variable - 'status'
status = pd.get_dummies(house_price_dataset['furnishingstatus'])

# Check what the dataset 'status' looks like
status.head()

"""Now, we don't need three columns. we can drop the furnished column, as the type of furnishing can be identified with just the last two columns where â€”



*   00 will correspond to furnished
*   01 will correspond to unfurnished
*   10 will correspond to semi-furnished




"""

# Let's drop the first column from status df using 'drop_first = True'

status = pd.get_dummies(house_price_dataset['furnishingstatus'], drop_first = True)

# Add the results to the original housing dataframe

housing = pd.concat([house_price_dataset, status], axis = 1)

# Now let's see the head of our dataframe.

housing.head()

# Drop 'furnishingstatus' as we have created the dummies for it

housing.drop(['furnishingstatus'], axis = 1, inplace = True)

housing.head()

"""#Splitting the Data into Training and Testing Sets"""

from sklearn.model_selection import train_test_split

# We specify this so that the train and test data set always have the same rows, respectively
np.random.seed(0)
df_train, df_test = train_test_split(housing, train_size = 0.7, test_size = 0.3, random_state = 100)

print(df_train.columns)
print(df_train.shape)

print(df_test.columns)
print(df_test.shape)

"""#Rescaling the Features
As we saw in the demonstration for Simple Linear Regression, scaling doesn't impact your model. Here we can see that except for area, all the columns have small integer values. So it is extremely important to rescale the variables so that they have a comparable scale. If we don't have comparable scales, then some of the coefficients as obtained by fitting the regression model might be very large or very small as compared to the other coefficients. This might become very annoying at the time of model evaluation. So it is advised to use standardization or normalization so that the units of the coefficients obtained are all on the same scale. As we know, there are two common ways of rescaling:


Min-Max scaling
Standardisation (mean-0, sigma-1)

This time, we will use MinMax scaling.
"""

from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()

# Apply scaler() to all the columns except the 'yes-no' and 'dummy' variables
num_vars = ['area', 'bedrooms', 'bathrooms', 'stories', 'parking','price']

df_train[num_vars] = scaler.fit_transform(df_train[num_vars])

print(df_test.columns)
print(df_train.columns)

df_train.describe()

# Let's check the correlation coefficients to see which variables are highly correlated

plt.figure(figsize = (16, 10))
sns.heatmap(df_train.corr(), annot = True, cmap="YlGnBu")
plt.show()

"""#Dividing into X and Y sets for the model building"""

y_train = df_train.pop('price')
X_train = df_train
y_test  = df_test.pop('price')
X_test  = df_test

"""#Model Building

This time, we will be using the LinearRegression function from SciKit Learn for its compatibility with RFE (which is a utility from sklearn)
"""

# Importing r2 score, mean_square_error and LinearRegression
from sklearn.linear_model import LinearRegression

regressor=LinearRegression()

"""#Fitting trained data into our model"""

regressor.fit(X_train,y_train)

"""# Make a prediction of tested data"""

ypre=regressor.predict(X_test)
print(ypre)

"""# Find mean_square_error and r2score"""

mserror=mean_squared_error(y_test,ypre)
r2score=r2_score(y_test,ypre)
print("Mean square error is : ",mserror)
print("r2 score is : ", r2score)

"""#Printing y_test v/s ypre"""

fig = plt.figure()
plt.scatter(y_test,ypre)
fig.suptitle('y_test vs ypre', fontsize=20)              # Plot heading
plt.xlabel('y_test', fontsize=18)                          # X-label
plt.ylabel('y_pred', fontsize=16)